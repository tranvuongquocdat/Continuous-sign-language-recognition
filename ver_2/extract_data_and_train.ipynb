{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Khởi tạo MediaPipe Pose\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose()\n",
    "\n",
    "# Hàm để vẽ keypoints\n",
    "def draw_keypoints(image, keypoints, keypoint_ids):\n",
    "    tmp_lst = []\n",
    "    for idx, keypoint in enumerate(keypoints):\n",
    "        if idx in keypoint_ids:\n",
    "            x = int(keypoint.x * image.shape[1])\n",
    "            y = int(keypoint.y * image.shape[0])\n",
    "            cv2.circle(image, (x, y), 5, (0, 255, 0), -1)\n",
    "            tmp_lst.append([x, y])\n",
    "\n",
    "    return tmp_lst\n",
    "\n",
    "# Mở video\n",
    "cap = cv2.VideoCapture(r'crop_video\\017_002_003_cropped.mp4')\n",
    "lst = []\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Thay đổi kích thước frame xuống một nửa\n",
    "    frame = cv2.resize(frame, (frame.shape[1] // 2, frame.shape[0] // 2))\n",
    "\n",
    "    # Xử lý frame\n",
    "    results = pose.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    if results.pose_landmarks:\n",
    "        # Hiển thị các keypoints bạn chọn (ví dụ: 0, 1, 2)\n",
    "        tmp_lst = draw_keypoints(frame, results.pose_landmarks.landmark, [15, 17, 19, 21, 16, 18, 20, 22, 0, 2, 5])\n",
    "        lst.append(tmp_lst)\n",
    "\n",
    "    cv2.imshow('Keypoints in Video', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == 27:  # Nhấn ESC để thoát\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[239, 177], [261, 156], [218, 157], [377, 550], [188, 428], [387, 592], [224, 432], [368, 581], [230, 411], [363, 560], [219, 406]], [[240, 175], [261, 155], [218, 156], [377, 551], [185, 427], [387, 594], [221, 431], [368, 588], [228, 410], [363, 567], [217, 406]], [[240, 174], [261, 154], [218, 155], [377, 551], [183, 427], [386, 595], [219, 431], [368, 591], [226, 410], [362, 570], [216, 406]], [[240, 173], [261, 154], [218, 154], [377, 552], [182, 427], [385, 599], [218, 430], [367, 594], [225, 409], [361, 572], [215, 406]], [[240, 173], [261, 154], [218, 154], [377, 554], [182, 427], [385, 602], [217, 430], [367, 596], [225, 409], [361, 574], [214, 406]], [[240, 173], [261, 154], [218, 154], [377, 556], [181, 427], [384, 605], [217, 430], [367, 599], [225, 409], [361, 578], [214, 406]], [[240, 173], [261, 154], [218, 154], [377, 556], [181, 427], [384, 605], [217, 429], [367, 598], [225, 409], [361, 577], [214, 405]], [[240, 173], [261, 154], [218, 154], [376, 556], [181, 427], [384, 605], [217, 429], [367, 598], [225, 409], [361, 577], [214, 405]], [[240, 173], [261, 154], [218, 155], [376, 556], [181, 427], [385, 605], [217, 429], [367, 598], [225, 409], [361, 577], [214, 405]], [[240, 173], [261, 154], [218, 155], [376, 556], [181, 426], [385, 604], [217, 429], [367, 598], [225, 409], [361, 577], [214, 405]], [[240, 173], [261, 154], [218, 155], [377, 556], [181, 426], [385, 605], [217, 429], [367, 598], [225, 409], [361, 577], [215, 405]], [[240, 173], [261, 154], [218, 155], [377, 557], [181, 426], [385, 609], [217, 429], [367, 601], [225, 409], [361, 579], [215, 405]], [[240, 173], [261, 154], [219, 155], [377, 559], [181, 426], [385, 611], [217, 429], [367, 604], [226, 409], [361, 583], [215, 405]], [[240, 173], [261, 154], [219, 155], [377, 559], [181, 426], [385, 611], [217, 429], [367, 604], [226, 409], [361, 584], [215, 405]], [[240, 173], [261, 154], [219, 155], [377, 559], [181, 426], [385, 611], [217, 429], [367, 604], [226, 409], [361, 583], [215, 405]], [[240, 173], [261, 154], [219, 155], [377, 559], [182, 425], [385, 611], [217, 429], [367, 604], [226, 409], [361, 583], [215, 405]], [[240, 173], [261, 154], [219, 155], [377, 558], [182, 425], [385, 609], [217, 429], [366, 603], [226, 409], [361, 581], [215, 405]], [[240, 173], [261, 154], [219, 155], [377, 558], [182, 425], [385, 607], [217, 429], [366, 602], [226, 409], [361, 580], [215, 405]], [[240, 173], [261, 154], [219, 155], [377, 557], [182, 425], [384, 606], [217, 429], [366, 601], [226, 409], [360, 578], [216, 405]], [[240, 173], [261, 154], [219, 155], [377, 557], [182, 425], [384, 605], [218, 429], [365, 599], [226, 409], [360, 577], [216, 405]], [[240, 173], [261, 154], [219, 155], [377, 556], [182, 425], [384, 604], [218, 429], [365, 599], [226, 409], [360, 576], [216, 405]], [[240, 173], [261, 154], [219, 155], [377, 556], [182, 425], [384, 603], [218, 429], [365, 597], [226, 409], [360, 575], [216, 405]], [[240, 173], [261, 154], [219, 155], [377, 556], [182, 425], [384, 602], [218, 429], [365, 596], [226, 409], [360, 574], [216, 405]], [[240, 173], [261, 154], [219, 155], [377, 555], [182, 425], [383, 601], [218, 429], [365, 595], [226, 409], [360, 573], [216, 405]], [[240, 173], [261, 154], [219, 155], [377, 555], [182, 425], [383, 601], [218, 429], [365, 594], [226, 409], [360, 572], [216, 405]], [[240, 174], [261, 154], [219, 156], [377, 555], [182, 425], [383, 601], [218, 429], [365, 594], [226, 409], [360, 572], [216, 405]], [[240, 174], [261, 154], [219, 156], [377, 555], [182, 425], [383, 601], [218, 429], [365, 594], [226, 409], [360, 572], [216, 404]], [[240, 174], [261, 154], [219, 156], [377, 555], [182, 425], [383, 601], [218, 429], [365, 594], [226, 409], [360, 572], [216, 404]], [[240, 174], [261, 154], [219, 156], [377, 555], [182, 425], [383, 601], [218, 429], [365, 595], [226, 409], [360, 572], [216, 404]], [[240, 174], [261, 154], [219, 156], [377, 555], [182, 424], [383, 602], [217, 428], [365, 595], [226, 408], [360, 573], [215, 404]], [[240, 174], [261, 154], [219, 156], [377, 555], [182, 424], [383, 602], [217, 427], [365, 596], [225, 406], [360, 573], [215, 402]], [[240, 174], [261, 154], [219, 156], [377, 555], [182, 423], [383, 603], [217, 426], [365, 597], [225, 406], [360, 574], [215, 402]], [[240, 174], [261, 154], [219, 156], [377, 556], [182, 423], [383, 606], [217, 426], [365, 598], [225, 406], [360, 575], [215, 402]], [[240, 174], [261, 154], [219, 156], [377, 557], [182, 422], [383, 607], [217, 425], [365, 599], [225, 405], [360, 576], [215, 401]], [[240, 174], [261, 154], [219, 156], [377, 558], [181, 421], [383, 608], [217, 425], [364, 600], [225, 404], [360, 577], [214, 400]], [[240, 174], [261, 155], [220, 156], [377, 558], [181, 419], [383, 608], [215, 423], [364, 600], [224, 402], [360, 577], [213, 398]], [[240, 175], [261, 155], [220, 156], [377, 558], [179, 416], [382, 609], [214, 420], [364, 600], [223, 398], [360, 577], [212, 395]], [[240, 175], [261, 155], [220, 157], [377, 559], [178, 413], [382, 609], [214, 419], [364, 600], [222, 397], [360, 578], [212, 393]], [[240, 176], [261, 155], [220, 157], [377, 559], [177, 408], [382, 609], [214, 416], [364, 600], [222, 395], [360, 578], [212, 390]], [[241, 176], [261, 156], [220, 158], [377, 559], [176, 402], [382, 608], [213, 409], [364, 600], [222, 387], [360, 578], [211, 383]], [[241, 177], [261, 156], [220, 158], [377, 558], [175, 393], [382, 607], [212, 401], [365, 600], [221, 378], [360, 578], [210, 374]], [[241, 177], [261, 156], [221, 159], [377, 555], [175, 388], [384, 602], [212, 395], [367, 594], [220, 373], [362, 575], [210, 369]], [[242, 178], [261, 156], [221, 159], [378, 553], [170, 376], [385, 598], [207, 378], [368, 590], [216, 359], [363, 573], [207, 358]], [[242, 178], [261, 156], [221, 159], [378, 551], [167, 365], [385, 597], [204, 366], [369, 589], [213, 350], [363, 572], [205, 348]], [[242, 178], [262, 157], [221, 159], [378, 550], [163, 355], [385, 594], [201, 354], [369, 586], [209, 340], [363, 569], [202, 338]], [[244, 178], [262, 157], [222, 159], [378, 549], [160, 343], [385, 592], [199, 339], [369, 583], [206, 322], [363, 566], [200, 320]], [[244, 178], [262, 157], [222, 159], [378, 549], [157, 332], [385, 590], [198, 330], [369, 580], [205, 313], [363, 563], [198, 311]], [[245, 179], [262, 157], [223, 159], [378, 548], [153, 319], [384, 587], [192, 315], [369, 577], [199, 300], [363, 560], [192, 300]], [[246, 179], [262, 157], [223, 159], [378, 547], [154, 308], [384, 585], [194, 300], [369, 575], [201, 288], [364, 559], [193, 289]], [[247, 181], [263, 157], [223, 160], [378, 547], [154, 289], [384, 585], [192, 278], [369, 575], [199, 269], [364, 559], [193, 272]], [[247, 182], [263, 157], [223, 161], [378, 547], [154, 279], [384, 586], [193, 269], [369, 575], [200, 262], [364, 559], [193, 266]], [[247, 183], [263, 158], [224, 161], [378, 547], [150, 268], [384, 586], [189, 255], [369, 576], [197, 250], [363, 559], [190, 254]], [[249, 184], [263, 158], [224, 163], [378, 547], [149, 256], [384, 586], [186, 245], [369, 576], [194, 239], [363, 559], [188, 244]], [[250, 184], [263, 158], [225, 163], [379, 548], [147, 244], [384, 586], [183, 230], [369, 575], [191, 227], [364, 558], [185, 233]], [[252, 185], [264, 159], [226, 164], [379, 548], [146, 228], [384, 586], [179, 212], [369, 574], [187, 209], [364, 558], [182, 217]], [[254, 186], [264, 161], [227, 165], [379, 548], [146, 218], [384, 588], [178, 200], [369, 574], [186, 199], [364, 558], [182, 207]], [[256, 187], [265, 162], [229, 166], [379, 549], [146, 211], [384, 589], [178, 192], [368, 575], [187, 193], [363, 558], [182, 202]], [[258, 188], [266, 163], [231, 167], [379, 549], [145, 210], [384, 589], [177, 191], [368, 575], [186, 192], [363, 558], [181, 200]], [[258, 188], [267, 164], [232, 167], [379, 549], [146, 205], [384, 589], [177, 186], [368, 574], [186, 188], [363, 558], [181, 197]], [[258, 188], [267, 164], [232, 167], [379, 549], [146, 205], [384, 589], [177, 190], [368, 574], [186, 191], [363, 558], [181, 199]], [[258, 189], [267, 164], [232, 167], [379, 549], [145, 205], [384, 589], [176, 191], [368, 574], [184, 192], [363, 557], [180, 200]], [[258, 189], [267, 165], [232, 168], [379, 548], [144, 205], [384, 589], [176, 191], [368, 574], [184, 193], [363, 557], [179, 200]], [[258, 189], [267, 165], [233, 168], [379, 548], [143, 205], [384, 589], [174, 191], [368, 574], [183, 193], [363, 557], [177, 201]], [[258, 189], [268, 165], [233, 168], [379, 548], [141, 204], [384, 589], [173, 189], [368, 574], [181, 192], [363, 558], [176, 199]], [[258, 189], [268, 165], [233, 168], [379, 548], [139, 204], [384, 589], [172, 190], [369, 575], [180, 192], [364, 558], [174, 200]], [[258, 189], [268, 165], [233, 168], [379, 548], [136, 205], [384, 588], [170, 191], [369, 575], [178, 193], [364, 558], [173, 200]], [[258, 190], [268, 165], [233, 168], [379, 548], [137, 204], [384, 588], [171, 190], [369, 575], [178, 192], [364, 558], [173, 199]], [[258, 190], [268, 165], [234, 168], [380, 548], [137, 204], [374, 587], [171, 190], [369, 576], [179, 191], [364, 558], [173, 199]], [[259, 190], [269, 165], [235, 168], [380, 548], [137, 204], [376, 587], [171, 190], [369, 576], [179, 191], [364, 559], [173, 199]], [[259, 190], [269, 165], [236, 168], [380, 548], [137, 203], [378, 587], [171, 190], [369, 576], [179, 191], [364, 559], [173, 199]], [[259, 190], [270, 166], [236, 168], [380, 548], [137, 203], [379, 587], [171, 190], [369, 576], [179, 192], [364, 559], [173, 199]], [[260, 190], [270, 166], [236, 168], [380, 548], [137, 203], [380, 587], [171, 190], [369, 576], [179, 192], [364, 559], [173, 199]], [[260, 190], [270, 166], [237, 168], [380, 548], [137, 203], [380, 587], [171, 190], [369, 576], [179, 192], [364, 559], [173, 199]], [[260, 190], [271, 166], [237, 168], [380, 548], [138, 203], [381, 587], [172, 190], [369, 576], [179, 191], [364, 558], [174, 198]], [[260, 191], [271, 166], [237, 168], [380, 548], [140, 202], [382, 587], [173, 189], [369, 576], [180, 191], [364, 558], [174, 197]], [[260, 191], [271, 166], [237, 168], [380, 548], [140, 202], [382, 587], [173, 189], [369, 576], [180, 191], [364, 558], [175, 198]], [[260, 191], [271, 167], [237, 168], [380, 548], [141, 202], [383, 587], [173, 189], [369, 576], [180, 191], [365, 558], [175, 198]], [[260, 191], [271, 167], [237, 168], [381, 548], [142, 202], [383, 587], [174, 189], [369, 576], [181, 191], [365, 558], [176, 198]], [[260, 191], [271, 167], [237, 169], [381, 548], [142, 202], [384, 588], [174, 189], [370, 577], [181, 191], [365, 559], [176, 198]], [[260, 191], [271, 167], [237, 169], [382, 548], [142, 202], [385, 588], [174, 189], [370, 577], [181, 191], [365, 560], [176, 198]], [[260, 192], [271, 167], [237, 169], [382, 548], [143, 202], [385, 589], [174, 189], [370, 577], [181, 191], [365, 560], [176, 198]], [[260, 192], [271, 168], [237, 169], [382, 548], [143, 202], [385, 589], [174, 189], [370, 578], [181, 191], [365, 560], [176, 198]], [[260, 192], [271, 168], [237, 170], [382, 549], [143, 202], [385, 590], [174, 189], [370, 579], [181, 191], [365, 560], [176, 198]], [[260, 192], [271, 168], [237, 170], [382, 550], [143, 202], [386, 593], [174, 189], [370, 584], [181, 191], [365, 563], [176, 198]], [[260, 192], [271, 168], [237, 170], [382, 550], [143, 202], [386, 594], [174, 189], [371, 586], [181, 191], [366, 564], [176, 198]], [[260, 193], [271, 168], [236, 170], [382, 550], [143, 202], [386, 594], [173, 189], [371, 587], [181, 191], [366, 565], [176, 198]], [[260, 192], [271, 168], [236, 170], [382, 551], [143, 202], [387, 594], [173, 189], [372, 588], [181, 191], [366, 565], [176, 198]], [[260, 192], [270, 168], [236, 170], [382, 551], [143, 202], [387, 595], [173, 189], [373, 589], [181, 191], [367, 566], [176, 198]]]\n"
     ]
    }
   ],
   "source": [
    "print(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2982/2982 [2:18:51<00:00,  2.79s/it]  \n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import os\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Khởi tạo MediaPipe Pose\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose()\n",
    "\n",
    "# Hàm để vẽ và lấy keypoints\n",
    "def draw_keypoints(image, keypoints, keypoint_ids):\n",
    "    tmp_lst = []\n",
    "    for idx, keypoint in enumerate(keypoints):\n",
    "        if idx in keypoint_ids:\n",
    "            x = int(keypoint.x * image.shape[1])\n",
    "            y = int(keypoint.y * image.shape[0])\n",
    "            tmp_lst.append([x, y])\n",
    "    return tmp_lst\n",
    "\n",
    "# Hàm để xử lý và lưu keypoints của một video vào CSV\n",
    "def process_and_save_video(video_path, extracted_data_folder, keypoint_ids):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    all_keypoints = []\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        results = pose.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "        if results.pose_landmarks:\n",
    "            keypoints = draw_keypoints(frame, results.pose_landmarks.landmark, keypoint_ids)\n",
    "            all_keypoints.append(keypoints)\n",
    "\n",
    "    cap.release()\n",
    "    # Lưu keypoints vào CSV\n",
    "    video_name = os.path.basename(video_path).split('.')[0]\n",
    "    csv_path = os.path.join(extracted_data_folder, video_name + '.csv')\n",
    "    pd.DataFrame(all_keypoints).to_csv(csv_path, index=False)\n",
    "\n",
    "# Đường dẫn đến folder chứa video\n",
    "video_folder_path = 'reframe_video'\n",
    "\n",
    "# Thư mục để lưu trữ file CSV\n",
    "extracted_data_folder = 'extracted_data'\n",
    "if not os.path.exists(extracted_data_folder):\n",
    "    os.makedirs(extracted_data_folder)\n",
    "\n",
    "# ID của keypoints mà bạn quan tâm\n",
    "keypoint_ids = [15, 17, 19, 21, 16, 18, 20, 22, 0, 2, 5]\n",
    "\n",
    "# Tạo danh sách đường dẫn video\n",
    "video_paths = [os.path.join(video_folder_path, video_file) for video_file in os.listdir(video_folder_path) if video_file.endswith('.mp4')]\n",
    "\n",
    "# Xử lý các video và lưu vào CSV sử dụng đa luồng\n",
    "with ThreadPoolExecutor(max_workers=1) as executor:\n",
    "    list(tqdm(executor.map(lambda p: process_and_save_video(p, extracted_data_folder, keypoint_ids), video_paths), total=len(video_paths)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2982/2982 [00:10<00:00, 276.35it/s]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Helper function to convert string keypoints to float lists\n",
    "def convert_keypoints(kp_str):\n",
    "    kp_str = kp_str.strip('[]')\n",
    "    return np.array(kp_str.split(',')).astype(float)\n",
    "\n",
    "# Tải và chuẩn bị dữ liệu\n",
    "data_folder = 'extracted_data'\n",
    "X, y_raw = [], []\n",
    "label_mapping = {}\n",
    "for csv_file in tqdm(os.listdir(data_folder)):\n",
    "    try:\n",
    "        if csv_file.endswith('.csv'):\n",
    "            label = int(csv_file.split('_')[0])  # Lấy label từ tên file\n",
    "            if label not in label_mapping:\n",
    "                label_mapping[label] = len(label_mapping)  # Map label to a unique index\n",
    "            mapped_label = label_mapping[label]\n",
    "            df = pd.read_csv(os.path.join(data_folder, csv_file), converters={i: convert_keypoints for i in range(11)})\n",
    "            keypoints = np.array(df.values.tolist())  # Convert DataFrame rows to list of keypoints\n",
    "            X.append(keypoints)\n",
    "            y_raw.append(mapped_label)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {csv_file}: {e}\")\n",
    "\n",
    "# Padding các chuỗi dữ liệu\n",
    "max_length = max(len(sequence) for sequence in X)\n",
    "X_pad = pad_sequences(X, maxlen=max_length, padding='post', dtype='float32')\n",
    "\n",
    "# Chia dữ liệu thành tập huấn luyện và kiểm thử\n",
    "X_train, X_test, y_train_raw, y_test_raw = train_test_split(X_pad, y_raw, test_size=0.2, random_state=42)\n",
    "\n",
    "# Get the number of classes based on the unique labels found\n",
    "num_classes = len(np.unique(y_raw))\n",
    "\n",
    "# Convert the raw labels to categorical\n",
    "y_train_cat = to_categorical(y_train_raw, num_classes=num_classes)\n",
    "y_test_cat = to_categorical(y_test_raw, num_classes=num_classes)\n",
    "\n",
    "# Reshape X_train and X_test to have the shape (num_samples, timesteps, num_features)\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], -1))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n"
     ]
    }
   ],
   "source": [
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "75/75 [==============================] - ETA: 0s - loss: 1.4695 - accuracy: 0.4797\n",
      "Epoch 1: val_accuracy improved from -inf to 0.64154, saving model to model\\model_checkpoint_1.h5\n",
      "75/75 [==============================] - 5s 36ms/step - loss: 1.4695 - accuracy: 0.4797 - val_loss: 0.9469 - val_accuracy: 0.6415\n",
      "Epoch 2/100\n",
      "73/75 [============================>.] - ETA: 0s - loss: 0.9845 - accuracy: 0.6426\n",
      "Epoch 2: val_accuracy improved from 0.64154 to 0.71524, saving model to model\\model_checkpoint_1.h5\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.9792 - accuracy: 0.6444 - val_loss: 0.8252 - val_accuracy: 0.7152\n",
      "Epoch 3/100\n",
      "73/75 [============================>.] - ETA: 0s - loss: 0.8359 - accuracy: 0.7016\n",
      "Epoch 3: val_accuracy improved from 0.71524 to 0.80737, saving model to model\\model_checkpoint_1.h5\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.8346 - accuracy: 0.7019 - val_loss: 0.6082 - val_accuracy: 0.8074\n",
      "Epoch 4/100\n",
      "73/75 [============================>.] - ETA: 0s - loss: 0.6778 - accuracy: 0.7735\n",
      "Epoch 4: val_accuracy did not improve from 0.80737\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.6873 - accuracy: 0.7711 - val_loss: 0.9466 - val_accuracy: 0.6382\n",
      "Epoch 5/100\n",
      "73/75 [============================>.] - ETA: 0s - loss: 0.6731 - accuracy: 0.7573\n",
      "Epoch 5: val_accuracy did not improve from 0.80737\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.6684 - accuracy: 0.7597 - val_loss: 0.6566 - val_accuracy: 0.7772\n",
      "Epoch 6/100\n",
      "73/75 [============================>.] - ETA: 0s - loss: 0.5930 - accuracy: 0.7864\n",
      "Epoch 6: val_accuracy did not improve from 0.80737\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.5951 - accuracy: 0.7857 - val_loss: 0.6927 - val_accuracy: 0.7688\n",
      "Epoch 7/100\n",
      "73/75 [============================>.] - ETA: 0s - loss: 0.5851 - accuracy: 0.8014\n",
      "Epoch 7: val_accuracy improved from 0.80737 to 0.84757, saving model to model\\model_checkpoint_1.h5\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.5850 - accuracy: 0.8004 - val_loss: 0.3755 - val_accuracy: 0.8476\n",
      "Epoch 8/100\n",
      "73/75 [============================>.] - ETA: 0s - loss: 0.6998 - accuracy: 0.7406\n",
      "Epoch 8: val_accuracy did not improve from 0.84757\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.7018 - accuracy: 0.7417 - val_loss: 0.7073 - val_accuracy: 0.6918\n",
      "Epoch 9/100\n",
      "73/75 [============================>.] - ETA: 0s - loss: 0.7933 - accuracy: 0.7145\n",
      "Epoch 9: val_accuracy did not improve from 0.84757\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.7948 - accuracy: 0.7136 - val_loss: 0.6910 - val_accuracy: 0.7387\n",
      "Epoch 10/100\n",
      "73/75 [============================>.] - ETA: 0s - loss: 0.6928 - accuracy: 0.7423\n",
      "Epoch 10: val_accuracy did not improve from 0.84757\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.7009 - accuracy: 0.7405 - val_loss: 1.0284 - val_accuracy: 0.6801\n",
      "Epoch 11/100\n",
      "73/75 [============================>.] - ETA: 0s - loss: 0.7544 - accuracy: 0.7205\n",
      "Epoch 11: val_accuracy did not improve from 0.84757\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.7552 - accuracy: 0.7203 - val_loss: 0.5839 - val_accuracy: 0.7906\n",
      "Epoch 12/100\n",
      "73/75 [============================>.] - ETA: 0s - loss: 0.5396 - accuracy: 0.8074\n",
      "Epoch 12: val_accuracy did not improve from 0.84757\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.5441 - accuracy: 0.8059 - val_loss: 0.5680 - val_accuracy: 0.7956\n",
      "Epoch 13/100\n",
      "73/75 [============================>.] - ETA: 0s - loss: 0.5913 - accuracy: 0.7838\n",
      "Epoch 13: val_accuracy did not improve from 0.84757\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.5996 - accuracy: 0.7816 - val_loss: 0.7747 - val_accuracy: 0.7471\n",
      "Epoch 14/100\n",
      "73/75 [============================>.] - ETA: 0s - loss: 0.7122 - accuracy: 0.7547\n",
      "Epoch 14: val_accuracy did not improve from 0.84757\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.7092 - accuracy: 0.7568 - val_loss: 0.8369 - val_accuracy: 0.7538\n",
      "Epoch 15/100\n",
      "73/75 [============================>.] - ETA: 0s - loss: 0.6897 - accuracy: 0.7646\n",
      "Epoch 15: val_accuracy did not improve from 0.84757\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.6980 - accuracy: 0.7614 - val_loss: 1.1838 - val_accuracy: 0.6281\n",
      "Epoch 16/100\n",
      "73/75 [============================>.] - ETA: 0s - loss: 0.9719 - accuracy: 0.6759\n",
      "Epoch 16: val_accuracy did not improve from 0.84757\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.9665 - accuracy: 0.6784 - val_loss: 0.6182 - val_accuracy: 0.8157\n",
      "Epoch 17/100\n",
      "73/75 [============================>.] - ETA: 0s - loss: 0.5721 - accuracy: 0.8039\n",
      "Epoch 17: val_accuracy did not improve from 0.84757\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.5695 - accuracy: 0.8050 - val_loss: 0.4824 - val_accuracy: 0.8375\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "\n",
    "# Make sure you have executed the data preprocessing steps before running this code\n",
    "# X_train, X_test, y_train_cat, and y_test_cat should already be defined\n",
    "\n",
    "# Determine the number of classes for the output layer\n",
    "num_classes = y_train_cat.shape[1]\n",
    "\n",
    "# Xây dựng mô hình LSTM\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, activation = 'tanh', input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(LSTM(128, return_sequences=True, activation='tanh'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(LSTM(64, return_sequences=False, activation='tanh'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))  # Use the determined number of classes here\n",
    "\n",
    "# Define the Adam optimizer with the initial learning rate\n",
    "adam_optimizer = Adam(learning_rate=0.00001)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Tạo callback EarlyStopping\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "# Tạo callback để lưu mô hình với val_accuracy tốt nhất\n",
    "checkpoint_path = \"model/model_checkpoint_1.h5\"\n",
    "checkpoint = ModelCheckpoint(checkpoint_path, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "# Huấn luyện mô hình với callback\n",
    "history = model.fit(X_train, y_train_cat,batch_size=32, epochs=100, validation_data=(X_test, y_test_cat), callbacks=[checkpoint, early_stop], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_15 (LSTM)              (None, 120, 64)           22272     \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 120, 64)           0         \n",
      "                                                                 \n",
      " lstm_16 (LSTM)              (None, 120, 128)          98816     \n",
      "                                                                 \n",
      " dropout_16 (Dropout)        (None, 120, 128)          0         \n",
      "                                                                 \n",
      " lstm_17 (LSTM)              (None, 64)                49408     \n",
      "                                                                 \n",
      " dropout_17 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 10)                330       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 177,066\n",
      "Trainable params: 177,066\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 1s 25ms/step - loss: 0.4824 - accuracy: 0.8375\n",
      "Test Accuracy: 83.75%\n"
     ]
    }
   ],
   "source": [
    "# Đánh giá mô hình\n",
    "loss, accuracy = model.evaluate(X_test, y_test_cat)\n",
    "print(f\"Test Accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 501ms/step\n",
      "Hành động được dự đoán là: 17\n",
      "Hành động thực tế là: 017\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Hành động được dự đoán là: 21\n",
      "Hành động thực tế là: 021\n",
      "cannot read file 021_006_005_cropped_shrinking_linearly.csv\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Hành động được dự đoán là: 26\n",
      "Hành động thực tế là: 026\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Hành động được dự đoán là: 33\n",
      "Hành động thực tế là: 033\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Hành động được dự đoán là: 50\n",
      "Hành động thực tế là: 050\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Hành động được dự đoán là: 64\n",
      "Hành động thực tế là: 064\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Hành động được dự đoán là: 64\n",
      "Hành động thực tế là: 064\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def predict_action(csv_file_path, model, label_mapping):\n",
    "    df = pd.read_csv(csv_file_path, converters={i: convert_keypoints for i in range(11)})\n",
    "    keypoints = np.array(df.values.tolist())\n",
    "    keypoints_pad = pad_sequences([keypoints], maxlen=max_length, padding='post', dtype='float32').reshape((1, max_length, -1))\n",
    "    prediction = model.predict(keypoints_pad)\n",
    "    action_class = np.argmax(prediction)\n",
    "    # Reverse map to original label\n",
    "    original_label = {v: k for k, v in label_mapping.items()}[action_class]\n",
    "    return original_label\n",
    "\n",
    "# Dự đoán hành động từ một file CSV mới\n",
    "directory = r\"test_csv\" \n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    try:\n",
    "        if filename.endswith(\".csv\"):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            predicted_action = predict_action(file_path, model, label_mapping)\n",
    "            print(f\"Hành động được dự đoán là: {predicted_action}\\nHành động thực tế là: {filename.split('_')[0]}\")\n",
    "    except:\n",
    "        print(f\"cannot read file {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 363ms/step\n",
      "Predicted Action: 64\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Predicted Action: 44\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Predicted Action: 33\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Predicted Action: 64\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Predicted Action: 44\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Predicted Action: 51\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Predicted Action: 26\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Khởi tạo MediaPipe Pose\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose()\n",
    "\n",
    "# Hàm để trích xuất và lấy keypoints cần thiết\n",
    "def extract_keypoints(results, keypoint_ids):\n",
    "    keypoints = []\n",
    "    for idx in keypoint_ids:\n",
    "        keypoint = results.pose_landmarks.landmark[idx]\n",
    "        keypoints.extend([keypoint.x, keypoint.y, keypoint.z, keypoint.visibility])\n",
    "    return keypoints\n",
    "\n",
    "# Xử lý video từ webcam và trích xuất keypoints\n",
    "def process_webcam_stream(model, keypoint_ids, max_length):\n",
    "    cap = cv2.VideoCapture(r\"test_video\\real_test_1.mp4\")\n",
    "    sequence = []\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame = cv2.resize(frame, (960, 1080))\n",
    "        # Xử lý frame để trích xuất keypoints\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = pose.process(frame_rgb)\n",
    "\n",
    "        if results.pose_landmarks:\n",
    "            keypoints = extract_keypoints(results, keypoint_ids)\n",
    "            sequence.append(keypoints)\n",
    "\n",
    "            # Dự đoán hành động nếu đủ số lượng keypoints\n",
    "            if len(sequence) == max_length:\n",
    "                action = predict_action(sequence, model, max_length)\n",
    "                print(f\"Predicted Action: {action}\")\n",
    "                sequence = []  # Reset sequence sau mỗi dự đoán\n",
    "\n",
    "        cv2.imshow('Webcam Feed', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):  # Nhấn 'q' để thoát\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Hàm để dự đoán hành động từ chuỗi keypoints\n",
    "def predict_action(sequence, model, max_length):\n",
    "    sequence_pad = pad_sequences([sequence], maxlen=max_length, padding='post', dtype='float32')\n",
    "    prediction = model.predict(sequence_pad)\n",
    "    action_class = np.argmax(prediction)\n",
    "    original_label = {v: k for k, v in label_mapping.items()}[action_class]\n",
    "    return original_label\n",
    "\n",
    "# ID của keypoints mà bạn quan tâm\n",
    "keypoint_ids = [15, 17, 19, 21, 16, 18, 20, 22, 0, 2, 5]\n",
    "\n",
    "max_length = 120\n",
    "\n",
    "# Gọi hàm xử lý video từ webcam\n",
    "process_webcam_stream(model, keypoint_ids, max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 17, 1: 21, 2: 26, 3: 33, 4: 39, 5: 44, 6: 50, 7: 51, 8: 56, 9: 64}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{v: k for k, v in label_mapping.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Hàm để dự đoán hành động từ chuỗi keypoints\n",
    "def predict_action(sequence, model, max_length):\n",
    "    sequence_pad = pad_sequences([sequence], maxlen=max_length, padding='post', dtype='float32')\n",
    "    prediction = model.predict(sequence_pad)\n",
    "    action_class = np.argmax(prediction)\n",
    "    original_label = {v: k for k, v in label_mapping.items()}[action_class]\n",
    "    return original_label\n",
    "\n",
    "# 1. New detection variables\n",
    "sequence = []\n",
    "sentence = []\n",
    "predictions = []\n",
    "threshold = 0.5\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        print(results)\n",
    "        \n",
    "        # Draw landmarks\n",
    "        draw_styled_landmarks(image, results)\n",
    "        \n",
    "        # 2. Prediction logic\n",
    "        keypoints = extract_keypoints(results)\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-30:]\n",
    "        \n",
    "        if len(sequence) == 30:\n",
    "            res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "            print(actions[np.argmax(res)])\n",
    "            predictions.append(np.argmax(res))\n",
    "            \n",
    "            \n",
    "        #3. Viz logic\n",
    "            if np.unique(predictions[-10:])[0]==np.argmax(res): \n",
    "                if res[np.argmax(res)] > threshold: \n",
    "                    \n",
    "                    if len(sentence) > 0: \n",
    "                        if actions[np.argmax(res)] != sentence[-1]:\n",
    "                            sentence.append(actions[np.argmax(res)])\n",
    "                    else:\n",
    "                        sentence.append(actions[np.argmax(res)])\n",
    "\n",
    "            if len(sentence) > 5: \n",
    "                sentence = sentence[-5:]\n",
    "\n",
    "            # Viz probabilities\n",
    "            image = prob_viz(res, actions, image, colors)\n",
    "            \n",
    "        cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "        cv2.putText(image, ' '.join(sentence), (3,30), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Show to screen\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
